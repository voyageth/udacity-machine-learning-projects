{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report\n",
    "\n",
    "You will be required to submit a project report along with your modified agent code as part of your submission. \n",
    "As you complete the tasks below, include thorough, detailed answers to each question provided in italics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Basic Driving Agent\n",
    "\n",
    "To begin, your only task is to get the **smartcab** to move around in the environment. \n",
    "At this point, you will not be concerned with any sort of optimal driving policy. \n",
    "Note that the driving agent is given the following information at each intersection:\n",
    "\n",
    "- The next waypoint location relative to its current location and heading.\n",
    "- The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "- The current time left from the allotted deadline.\n",
    "\n",
    "To complete this task, \n",
    "simply have your driving agent choose a random action from the set of possible actions (`None`, `'forward'`, `'left'`, `'right'`) at each intersection, \n",
    "disregarding the input information above. \n",
    "Set the simulation deadline enforcement, `enforce_deadline` to `False` and observe how it performs.\n",
    "\n",
    "_**QUESTION:** Observe what you see with the agent's behavior as it takes random actions. \n",
    "Does the **smartcab** eventually make it to the destination? \n",
    "Are there any other interesting observations to note?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "- About 30%, random action smartcab can arrive at destination.\n",
    "- random action\n",
    "  - average distance : 4.56253423423\n",
    "  - average dealine : 29.0\n",
    "  - average steps : 25.89\n",
    "  - average total reward : 0.44\n",
    "  - average penalty : 6.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inform the Driving Agent\n",
    "\n",
    "Now that your driving agent is capable of moving around in the environment, \n",
    "your next task is to identify a set of states that are appropriate for modeling the **smartcab** and environment. \n",
    "The main source of state variables are the current inputs at the intersection, but not all may require representation. \n",
    "You may choose to explicitly define states, or use some combination of inputs as an implicit state. \n",
    "At each time step, process the inputs and update the agent's current state using the `self.state` variable. \n",
    "Continue with the simulation deadline enforcement `enforce_deadline` being set to `False`, \n",
    "and observe how your driving agent now reports the change in state as the simulation progresses.\n",
    "\n",
    "_**QUESTION:** What states have you identified that are appropriate for modeling the **smartcab** and environment? \n",
    "Why do you believe each of these states to be appropriate for this problem?_\n",
    "\n",
    "_**OPTIONAL:** How many states in total exist for the **smartcab** in this environment? \n",
    "Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? \n",
    "Why or why not?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ANSWER **\n",
    "\n",
    "What states have you identified that are appropriate for modeling the smartcab and environment? \n",
    "\n",
    "- Available environment values\n",
    "  - `light` = ['red', 'green']\n",
    "  - `oncoming` = [None, 'forward', 'left', 'right']\n",
    "  - `right` = [None, 'forward', 'left', 'right']\n",
    "  - `left` = [None, 'forward', 'left', 'right']\n",
    "  - `next_waypoint` = ['forward', 'left', 'right', None]\n",
    "- All available states count is 2 \\* 4 \\* 4 \\* 4 \\* 4 = 512\n",
    "- But we do not need all 512 states. I think following 16 states are appropriate for this problem.\n",
    "- I don't use `deadline` feature when select state. \n",
    "  - Because if smartcab has no time for reach to destination within deadline can be ignore rule of the road. \n",
    "  - So, I ignore deadline feature deliberately.\n",
    "- CanMove states. \n",
    "  - `light` = 'green'\n",
    "    - `GREEN_CAN_LEFT`\n",
    "      - `None`, `'forward'`, `'right'`, `'left'`\n",
    "    - `GREEN_CANT_LEFT`\n",
    "      - `None`, `'forward'`, `'right'`\n",
    "  - `light` = `red`\n",
    "    - `RED_CAN_RIGHT`\n",
    "      - `None`, `'right'`\n",
    "    - `RED_CANT_RIGHT`\n",
    "      - `None`\n",
    "- CanMove states describe our smartcab can go or not to some direction.\n",
    "  - It contains `light`, `oncoming`, `right`, `left` environment values.\n",
    "  - So, I think these values must included to available states.\n",
    "- And append `next_waypoint`.\n",
    "  - Because `next_waypoint` determine smartcab's next action.\n",
    "  - ex. If `next_waypoint` is not included to states, we can not determine action correctly.\n",
    "    - `GREEN_CAN_LEFT` state does not describe about which action should be selected.\n",
    "    - If next_waypoint is included to states, `GREEN_CAN_LEFT_NEXT_NONE` will be select `None` action.\n",
    "    - Otherwise, `GREEN_CAN_LEFT_NEXT_FORWARD` should be select `'forward'` action.\n",
    "- SO, I determine available_states as \n",
    "  - available_states = [\n",
    "            'GREEN_CAN_LEFT_NEXT_NONE'\n",
    "            , 'GREEN_CAN_LEFT_NEXT_FORWARD'\n",
    "            , 'GREEN_CAN_LEFT_NEXT_LEFT'\n",
    "            , 'GREEN_CAN_LEFT_NEXT_RIGHT'\n",
    "\n",
    "            , 'GREEN_CANT_LEFT_NEXT_NONE'\n",
    "            , 'GREEN_CANT_LEFT_NEXT_FORWARD'\n",
    "            , 'GREEN_CANT_LEFT_NEXT_LEFT'\n",
    "            , 'GREEN_CANT_LEFT_NEXT_RIGHT'\n",
    "\n",
    "            , 'RED_CAN_RIGHT_NEXT_NONE'\n",
    "            , 'RED_CAN_RIGHT_NEXT_FORWARD'\n",
    "            , 'RED_CAN_RIGHT_NEXT_LEFT'\n",
    "            , 'RED_CAN_RIGHT_NEXT_RIGHT'\n",
    "\n",
    "            , 'RED_CANT_RIGHT_NEXT_NONE'\n",
    "            , 'RED_CANT_RIGHT_NEXT_FORWARD'\n",
    "            , 'RED_CANT_RIGHT_NEXT_LEFT'\n",
    "            , 'RED_CANT_RIGHT_NEXT_RIGHT'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "- I think above 16 states can represent all possible actions.\n",
    "\n",
    "\n",
    "\n",
    "How many states in total exist for the smartcab in this environment?\n",
    "\n",
    "- 512\n",
    "\n",
    "Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n",
    "\n",
    "- No. too many redundant states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Q-Learning Driving Agent\n",
    "\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, \n",
    "your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, \n",
    "based on the Q-values for the current state and action. \n",
    "Each action taken by the **smartcab** will produce a reward which depends on the state of the environment. \n",
    "The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, \n",
    "set the simulation deadline enforcement `enforce_deadline` to `True`. \n",
    "Run the simulation and observe how the **smartcab** moves about the environment in each trial.\n",
    "\n",
    "The formulas for updating Q-values can be found in \n",
    "[this](https://classroom.udacity.com/nanodegrees/nd009/parts/0091345409/modules/e64f9a65-fdb5-4e60-81a9-72813beebb7e/lessons/5446820041/concepts/6348990570923) \n",
    "video.\n",
    "\n",
    "_**QUESTION:** What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? \n",
    "Why is this behavior occurring?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? \n",
    "\n",
    "- random action\n",
    "  - average distance : 4.56253423423\n",
    "  - average dealine : 29.0\n",
    "  - average steps : 25.89\n",
    "  - average total reward : 0.44\n",
    "  - average penalty : 6.91\n",
    "\n",
    "- initial using q-learning(training by random select by epsilon first 90 times, estimate last 10 times without random select.)\n",
    "  - average distance : 4.66289817\n",
    "  - average dealine : 29.9\n",
    "  - average steps : 13.21\n",
    "  - average total reward : 22.105\n",
    "  - average penalty count : 0\n",
    "\n",
    "\n",
    "- Q-learning smartcab can find the way to destinaiton. \n",
    "  - Going circles sometimes.\n",
    "  - Occasionally select random way due to epsilon.\n",
    "  - Following the rule of road(if turn of random select by epsilon.).\n",
    "  - It can find the way to destination.\n",
    "\n",
    "Why is this behavior occurring?\n",
    "\n",
    "- Smartcab learned by q-learning. And select appropriate way rather than dumb random way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the Q-Learning Driving Agent\n",
    "\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, \n",
    "the **smartcab** is able to reach the destination within the allotted time safely and efficiently. \n",
    "Parameters in the Q-Learning algorithm, such as the learning rate (`alpha`), \n",
    "the discount factor (`gamma`) and the exploration rate (`epsilon`) all contribute to the driving agent’s ability to learn the best action for each state.\n",
    "To improve on the success of your smartcab:\n",
    "\n",
    "- Set the number of trials, `n_trials`, in the simulation to 100.\n",
    "- Run the simulation with the deadline enforcement `enforce_deadline` set to `True`\n",
    "(you will need to reduce the update delay `update_delay` and set the `display` to `False`).\n",
    "- Observe the driving agent’s learning and **smartcab’s** success rate, particularly during the later trials.\n",
    "- Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully.\n",
    "\n",
    "_**QUESTION:** Report the different values for the parameters tuned in your basic implementation of Q-Learning. \n",
    "For which set of parameters does the agent perform best? \n",
    "How well does the final driving agent perform?_\n",
    "\n",
    "_**QUESTION:** Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? \n",
    "How would you describe an optimal policy for this problem?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ANSWER **\n",
    "\n",
    "Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? \n",
    "\n",
    "- Initialize q-values.\n",
    "  - default all q-values are zero.\n",
    "  - big penalty when ignore rule of road.\n",
    "  \n",
    "```\n",
    "        # big penalty when ignore rule of the road\n",
    "        big_penalty_value = -100\n",
    "        self.set_q_value('GREEN_CANT_LEFT_NEXT_NONE', 'left', big_penalty_value)\n",
    "        self.set_q_value('GREEN_CANT_LEFT_NEXT_FORWARD', 'left', big_penalty_value)\n",
    "        self.set_q_value('GREEN_CANT_LEFT_NEXT_LEFT', 'left', big_penalty_value)\n",
    "        self.set_q_value('GREEN_CANT_LEFT_NEXT_RIGHT', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_NONE', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_NONE', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_FORWARD', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_FORWARD', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_LEFT', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_LEFT', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_RIGHT', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CAN_RIGHT_NEXT_RIGHT', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_NONE', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_NONE', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_NONE', 'right', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_FORWARD', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_FORWARD', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_FORWARD', 'right', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_LEFT', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_LEFT', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_LEFT', 'right', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_RIGHT', 'forward', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_RIGHT', 'left', big_penalty_value)\n",
    "        self.set_q_value('RED_CANT_RIGHT_NEXT_RIGHT', 'right', big_penalty_value)\n",
    "```\n",
    "\n",
    "- I justify q-learning function like this.\n",
    "  - alpha = 1 / (learning_delta * n)\n",
    "  - used step = initial deadline - current deadline\n",
    "  - reward'\n",
    "    - if reward < -0.5 :\n",
    "      - reward' = (reward * 100) / max(used step, 1) * 10\n",
    "    - else:\n",
    "      - reward' = reward / max(used step, 1) * 10\n",
    "  - Q' = (1 - alpha) \\* Q + alpha \\* (reward' + gamma \\* max_q_value_in_state)\n",
    "- I use reward' instead of reward. Because i want force more penalty when ignore rule of the roads. And add some penalty on used step. As result, our smartcab gave less rewad when used step is large.\n",
    "- And find learning_delta, gamma, epsilon values which maximize average total reward.\n",
    "\n",
    "- test result\n",
    "<a href=\"20160819_smartcab_result.xlsx\">result xlsx file</a>\n",
    "\n",
    "- epsilon : 0.9 is best.\n",
    "<img src=\"images/20160819_smartcab_epsilon.PNG\"/>\n",
    "\n",
    "- gamma : 0.8 is best.\n",
    "<img src=\"images/20160819_smartcab_gamma.PNG\"/>\n",
    "\n",
    "- learning_delta : 0.2 is best.\n",
    "<img src=\"images/20160819_smartcab_learning_delta.PNG\"/>\n",
    "\n",
    "- best parameter set is\n",
    "  - alpha : 1/(0.2 * n)\n",
    "  - gamma : 0.8\n",
    "  - epsilon : 0.9\n",
    "\n",
    "How well does the final driving agent perform?\n",
    "\n",
    "- Maximum average total reward point : 42.4\n",
    "- Average average total reward point : 26.411\n",
    "\n",
    "How would you describe an optimal policy for this problem?\n",
    "\n",
    "- Optimal policy should be like this.\n",
    "  - Not incur any penalites.\n",
    "  - Average total reward is high.\n",
    "\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? \n",
    "\n",
    "- Result is like this.\n",
    "<img src=\"images/20160819_smartcab_result.PNG\"/>\n",
    "\n",
    "- Q-value's table is like this.\n",
    "\n",
    "| state                         | action : None | action : forward | action : left | action : right |\n",
    "|-------------------------------|---------------|------------------|---------------|----------------|\n",
    "| GREEN_CAN_LEFT_NEXT_NONE   \t| 0.000000|    0.000000|    0.000000|    0.000000|\n",
    "| GREEN_CAN_LEFT_NEXT_FORWARD   | 0.934968|    3.606986|    2.379418|    0.825169|\n",
    "| GREEN_CAN_LEFT_NEXT_LEFT   \t| 2.002181|    0.163618|    5.018174|    2.144785|\n",
    "| GREEN_CAN_LEFT_NEXT_RIGHT   \t| 1.944834|    4.670036|    1.786558|    4.495921|\n",
    "| GREEN_CANT_LEFT_NEXT_NONE   \t| 0.000000|    0.000000| -100.000000|    0.000000|\n",
    "| GREEN_CANT_LEFT_NEXT_FORWARD  | 0.000000|    0.000000| -100.000000|    0.002396|\n",
    "| GREEN_CANT_LEFT_NEXT_LEFT   \t| 0.000000|    0.000000| -100.000000|    0.000000|\n",
    "| GREEN_CANT_LEFT_NEXT_RIGHT   \t| 0.000000|    0.000000|  -99.451756|    0.116174|\n",
    "| RED_CAN_RIGHT_NEXT_NONE   \t| 0.000000| -100.000000| -100.000000|    0.000000|\n",
    "| RED_CAN_RIGHT_NEXT_FORWARD   \t| 0.000000|  -17.628662|  -39.224885|   -0.580188|\n",
    "| RED_CAN_RIGHT_NEXT_LEFT  \t    | 0.377460|  -55.390485|  -25.718232|    2.041497|\n",
    "| RED_CAN_RIGHT_NEXT_RIGHT  \t| 5.776657|  -10.242352|    3.092213|   10.482273|\n",
    "| RED_CANT_RIGHT_NEXT_NONE  \t| 0.000000| -100.000000| -100.000000| -100.000000|\n",
    "| RED_CANT_RIGHT_NEXT_FORWARD  \t| 0.000000| -100.000000| -100.000000|  -95.797101|\n",
    "| RED_CANT_RIGHT_NEXT_LEFT  \t| 0.000000| -100.000000| -100.000000| -100.000000|\n",
    "| RED_CANT_RIGHT_NEXT_RIGHT  \t| 0.000000| -100.000000| -100.000000|  -99.744473|\n",
    "\n",
    "- Our smartcab always select action that not incur any penalties.\n",
    "- Average total reward is relatively high(compared to random selection).\n",
    "\n",
    "- random action\n",
    "  - average distance : 4.56253423423\n",
    "  - average dealine : 29.0\n",
    "  - average steps : 25.89\n",
    "  - average total reward : 0.44\n",
    "  - average penalty : 6.91\n",
    "\n",
    "- initial using q-learning\n",
    "  - average distance : 4.66289817\n",
    "  - average dealine : 29.9\n",
    "  - average steps : 13.21\n",
    "  - average total reward : 22.105\n",
    "  - average penalty count : 0\n",
    "\n",
    "- initial using q-learning\n",
    "  - average distance : 4.662993318\n",
    "  - average dealine : 29.895\n",
    "  - average steps : 15.448\n",
    "  - average total reward : 26.411\n",
    "  - average penalty count : 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
